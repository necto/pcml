\documentclass{article} % For LaTeX2e
% We will use NIPS submission format
\usepackage{nips13submit_e,times}
% for hyperlinks
%\usepackage{hyperref}
\usepackage{url}
% For figures
\usepackage{graphicx} 
\usepackage{subfigure} 
% math packages
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsopn}
\usepackage{ifthen}
\usepackage{natbib}
\usepackage{color}
\usepackage{float}
\usepackage{placeins}
\usepackage{geometry}

\title{Project-I by Group Istanbul}

\author{
Johan Droz\\
EPFL \\
\texttt{johan.droz@epfl.ch} \And
Arseniy Zaostrovnykh\\
EPFL \\
\texttt{arseniy.zaostrovnykh@epfl.ch}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\nipsfinalcopy 

\newcommand{\todo}[1]{}
\renewcommand{\todo}[1]{{\color{red} TODO: {#1}}}

\begin{document}

\maketitle

\begin{abstract}
In this report we describe our results for the first project of the 2015 PCML class.
We start by doing some basic exploratory data analysis on our dataset, then we implement regression and classification methods to analyze and predict data.
Finally we experiment with feature engineering and processing with the goal of improving the accuracy of our models.
\end{abstract}

\section{Data Description}

\paragraph{Regression} For the regression part of this project we received a dataset containing three sets named $\boldmath{X\_train, y\_train}$ and $\boldmath{X\_test}$. The pairs $\boldmath{X\_train, y\_train}$  form our training set. 
The former contains N = 2800 samples of dimensionality D = 66 and includes 54 real, 6 binary and 6 categorical variables with a number of categories between 3 and 4.
The later contains 2800 real variables that represent the output of $\boldmath{X\_train}$ .
We also get a test set $X\_test$ of 1200 data examples for which we have to predict the output $\boldmath{y}$ as well as the expected RMSE for our best model.

\paragraph{Classification} The dataset for the classification part of this project is quite similar to the first one.
There is also three sets named $\boldmath{X\_train, y\_train}$ and $\boldmath{X\_test}$. 
The pairs $\boldmath{X\_train, y\_train}$  form our training set. 
The former contains N = 1500 samples of dimensionality D = 24 and includes 20 real, 1 binary and 3 categorical variables with a number of categories varying between 4 and 5.
The later contains 1500 binary variables that represent the two classes $\{-1, 1\}$ in which the inputs belong.
We also get a test set $X\_test$ of 1200 data examples and our goal is to predict in which class they belong.

\section{Data visualization and cleaning}

The first step is to perform exploratory data analysis on the regression and classification datasets.

\emph{Note:} For all analyses that require cross-validation we use 7 folds. 

\paragraph{Regression} 
By looking at the histograms of the input variables, we notice that there are some outliers (41 to be exact). 
By removing them the RMSE decrease slightly.
For example with leastSquares it decreases from 1378.8 to 1370.4.

Secondly, we notice that the distributions of the input data are not centered, so we chose to normalize them.

Figure \ref{fig:scatter} show a scatter plot of one input variable vs the output and figure \ref{fig:histY} shows the histogram of the output.
We can notice that the data are separated into 3 groups.

We also look into the correlation between input variables and the output. We see that out of the 66 input variables, two have a high correlation with the output. There is variable number 42 and 57 (see figure \ref{fig:scatter}) which have respectively a correlation of 0.6216 and 0.8163.

The input matrix $\boldmath{X}$ is rank deficient with a rank of 55 instead of 66. This can lead to numerical issues when using methods such as least squares.
We can fight ill-conditioning with ridge regression. 

\paragraph{Classification}
The output range is $\{-1, 1\}$, to make it suitable for logistic regression, we apply a simple transformation: ${(y + 1)\over 2} \in \{0, 1\}$. Another observation is that the standard deviation of different dimensions varies significantly: from 0.5 upto 874, so we normalize the input data, to get rid of the scale bias. Finally we add a constant feature $X_0 \equiv 1$.

\begin{figure}[!t]
	\centering
	\subfigure[Scatter plot of one input variable vs output]{\label{fig:scatter}\includegraphics[width=0.55\textwidth]{figures/X58vsY.pdf}}
	\subfigure[Histogram of $\boldmath{y\_train}$]{\label{fig:histY}\includegraphics[width=0.4\textwidth]{figures/histY.pdf}}
	\caption{Data analysis}
\end{figure}

\section{Regression}
The implementation of least squares, least squares using gradient descent and ridge regression is mandatory for this project.

We start by applying those three methods to the data and visualize the results  as well as compute tests errors using cross validation.
To  estimate the RMSE of our model with the data in hand, we split them into training and validation sets and pretend validation set is the future data.

The best errors for each method are shown in Table \ref{tab:regression-errors}.

The rest of this section describes the details of each of these methods.

\begin{table}[!htb]
	\begin{minipage}{.5\linewidth}
		\centering

	\begin{center}
		\begin{tabular}{l|c}
			Method & RMSE  \\
			\hline
			Least Squares with Gradient Descent & 1370.1   \\
			Least Squares & 1370.1  \\
			Ridge regression &  1645 \\
			1-Feature Removal &  1408.9 \\
			X-Feature Removal &  1409 \\
			Dummy-coding &  1409.3 \\
		\end{tabular}
		\caption{The best errors for the regression methods tried.}
		\label{tab:regression-errors}
	\end{center}
	
	\end{minipage}
	\begin{minipage}{.5\linewidth}
		\centering
		
	\begin{center}
		\begin{tabular}{l|c}
			Feature\# & RMSE  \\
			\hline
			2 &  1410.2 \\
			12 & 1410.1 \\
			14 &  1409.4 \\
			29 &  1410.6 \\
			48 &  1410.2 \\
			62 &  1409.8 \\
		\end{tabular}
		\caption{RMSE for dummy coding.}
		\label{tab:regression-dummy}
	\end{center}
	
	\end{minipage} 
\end{table}


\subsection{Least Squares}
As explained in the ``Data visualization and cleaning" section, the matrix $\boldmath{X\_train}$ is ill-conditioned.
This is problematic with least squares because it can lead to wrong results.

In Table \ref{tab:regression-errors} we see that both Least Squares and Least Squares with gradient descent have the best RMSE even though the matrix is rank deficient.

\subsection{Ridge Regression}

For our dataset, Ridge regression performance is worse than Least Squares.
Figure \ref{fig:ridgeRegError} shows the results obtained with ridge regression for a polynomial of degree 2 and a training set of 90\% of the data, the rest being the testing set. $\lambda$ varies from $10$ to $10^{5}$ with a total of 20 points in between. 
There is only 20 points because the computation is expensive. Indeed with a degree 2, we have 4356 features and 287496 for degree 3.

\begin{figure}[!t]
	\center
	\includegraphics[width=0.8\textwidth]{figures/ridgeRegLoss.pdf}
	\caption{Plot of the test and training error for ridge regression}
	\label{fig:ridgeRegError}
\end{figure}


\subsection{Feature transformations}
In order to improve the RMSE of our model, we tried several feature transformations.

Firstly, we try to decrease the dimensionality of the problem by sequentially removing each feature and apply cross validation to estimate the RMSE with each feature removed.
Unfortunately, feature removal does not improve our model. It is actually the opposite as the RMSE increase slightly.

Secondly, we tried dummy coding of the categorical variables.
As explained in the ``Data description" section, the regression set's input has 6 categorical variables each with a number of categories between 3 and 4.
We apply dummy coding to each of these variables and compute the new RMSE.
Table \ref{tab:regression-dummy} shows that again there is no improvements.

\section{Classification}

Before getting into the analysis, we first shuffle the data by pseudo-random permutation to get rid of inherent ordering, that might bias the model. We also average the results over a range of random number generator seeds. We use In order to evaluate our final model later we cut off 10\% of the training data (150 points) to simulate the unseen future input.

We started with the classical logistic regression, and tried to improve the obtained result using penalized logistic regression and various feature transformations. The best errors for each method are shown in the Table~\ref{tab:classification-errors}. The rest of this section describes the details of each of them. The baseline -- trivial constant majority-based prediction is also shown for a reference.
\begin{table}[h]
  \begin{center}
    \begin{tabular}{l|ccc}
      Method & RMSE & 0-1 loss & logLoss \\
      \hline
      Majority & 0.69 & 0.47 & - \\
      Logistic Regression & 0.30 & 0.12 & 0.36 \\
      Penalized Logistic Regression & 0.35 & 0.15 & 0.35  \\
      1-Feature Removal & 0.34 & 0.17 & 0.36 \\
      Dummy-coding & 0.31 & 0.12 & 0.37 \\
      Polynomial & 0.34 & 0.18 & 0.45
    \end{tabular}
    \caption{The best errors for the classification methods tried.}
    \label{tab:classification-errors}
  \end{center}
\end{table}

\subsection{Penalized Logistic Regression}
As a natural next step after the logistic regression, we ran a penalized logistic regression, to see how much of the test error can be attributed to the variance in the data. The plot of logLoss is in the Fig.~\ref{fig:penLLmisses}. Other metrics evince a steady monotonic growth and have optimum at $\lambda = 0$, which tells us about high model bias and an insignificant variance in the data. Only the interesting range of $\lambda$ is shown.

\begin{figure}[h]
\center
\includegraphics[width=5in]{figures/penLLmisses.pdf}
\caption{Penalized logistic regression, ran on the original set of 24(+1) features.}
\label{fig:penLLmisses}
\end{figure}

\subsection{Feature transformations}
Look at the ``Data visualization and cleaning'' for the initial data preparation.

First thing we tried is {\bf removal} of every single feature, in oreder to decrease the dimensionality of the problem, to speed up future analyses. To find an irrelevant feature, we, using cross validation, sequentially fitted logistic regression on the training data set with the feature \# i removed, and tested the resulting model. Unfortunately, it turns out, that every of the 24 features is relevant, and removing any of them worsen the prediction, as you can see in the Table~\ref{tab:classification-errors}.

Having failed to reduct the feature set, we started expanding it, as our logistic regression has high bias. One of the promising methods is {\bf dummy-encoding}. There are four features, that have a finite and small discrete range. The results of expanding each of these features are shown in the Table~\ref{tab:classification-dummy}. As you can see from the table, no luck this time as well. Although, feature \#9 dummy-encoding promises a slightly (0.1\%) better prediction in terms of 0-1 loss, it is worse in other metrics, comparing to the original logistic regression.
\begin{table}[h]
  \begin{center}
    \begin{tabular}{l|ccc}
      Feature\# & RMSE & 0-1 loss & logLoss \\
      \hline
      8 & 0.3055 & 0.1233 & 0.3661 \\
      9 & 0.3051 & 0.1195 & 0.3654 \\
      10 & 0.3061 & 0.1219 & 0.3675 \\
      17 & 0.3060 & 0.1229 & 0.3659
    \end{tabular}
    \caption{The best errors for the classification methods tried.}
    \label{tab:classification-dummy}
  \end{center}
\end{table}

The last transformation tried is {\bf polynomial}. Monomial basis can theoretically approximate any function. We generate a full polynomial, which means 625 features for second degree and 15625 -- for the third, which is substentially greater than the number of training data points, so due to the curse of dimensionality, the regression is unable to fit a satisfactory model. Therefore we tested only the second degree. We used cross validation with penalized logistic regression to supress the high variance of 625 features. In the Fig.~\ref{fig:polyErrors} you can see the plot of logLoss function for the polynomial of the second degree. The other two metrics for the given random seed again do not have a bell form.

\begin{figure}[h]
\center
\includegraphics[width=5in]{figures/polyLogLoss.pdf}
\caption{Panalized logistic regression, ran on the expanded feature set: a second degree polynomial (625 features).}
\label{fig:polyErrors}
\end{figure}

\section{Summary}

In this report, we studied a number of classification and regression methods and found that logistic regression is the best fit for our classification problem and least squares gives the best results on the regression data. The table~\ref{tab:classification-errors} shows that you should prefer a penalized logistic regression only if you measure the logLoss, and the tried feature transformation did not improve the best model. We expect our predictions to be 12\% ($\pm$ 3\%) off on the test dataset.

The table \ref{tab:regression-errors} shows that least squares offer the lowest RMSE. But as the matrix is ill-conditioned we have to be careful because it can gives wrong results. 
We expect the RMSE of our predictions to be 1370.14.

\end{document}
