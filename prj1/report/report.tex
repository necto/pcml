\documentclass{article} % For LaTeX2e
% We will use NIPS submission format
\usepackage{nips13submit_e,times}
% for hyperlinks
\usepackage{hyperref}
\usepackage{url}
% For figures
\usepackage{graphicx} 
\usepackage{subfigure} 
% math packages
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsopn}
\usepackage{ifthen}
\usepackage{natbib}
\usepackage{color}

\title{Project-I by Group Istanbul}

\author{
Johan Droz\\
EPFL \\
\texttt{johan.droz@epfl.ch} \And
Arseniy Zaostrovnykh\\
EPFL \\
\texttt{arseniy.zaostrovnykh@epfl.ch}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\nipsfinalcopy 

\newcommand{\todo}[1]{}
\renewcommand{\todo}[1]{{\color{red} TODO: {#1}}}

\begin{document}

\maketitle

\begin{abstract}
\todo{Discuss and write down the chapter-wise structure of the report.}
\end{abstract}

\section{Data Description}
\todo{probably this one shold remain present.}

\section{Data visualization and cleaning}
\todo{normalization}
\todo{\{-1, 1\} to \{0, 1\}}
\todo{this works as an addition to the previous one.}
% A figure reference example: Figure \ref{fig:boxplotX}

\section{Regression}
\subsection{Feature transformations}
\section{Classification}

Before getting into the analysis, we first shuffle the data by pseudo-rundom permutation to get rid of inherent ordering, that might bias the model. We also average the results over a range of rundom number generator seeds. We use In order to evaluate our final model later we cut off 10\% of the training data (150 points) to simulate the unseen future input.

We started with the classical logistic regression \todo{Newton?}, and tried to improve the obtained result using penalized logistic regression and various feature transformations. The best errors for each method are shown in the Table~\ref{tab:classification-errors}. The rest of this section describes the details of each of them. The baseline -- trivial constant majority-based prediction is also shown for a reference.
\begin{table}[h]
  \begin{center}
    \begin{tabular}{l|ccc}
      Method & RMSE & 0-1 loss & logLoss \\
      \hline
      Majority & 0.69 & 0.47 & - \\
      Logistic Regression & 0.31 & 0.12 & 0.37 \\
      Penalized Logistic Regression & 0.34 & 0.15 & 0.36  \\
      1-Feature Removal & 0.34 & 0.17 & 0.36 \\
      Dummy-coding & 0.31 & 0.12 & 0.37 \\
      Polynomial & 0.33 & 0.17 & 0.46
    \end{tabular}
    \caption{The best errors for the classification methods tried.}
    \label{tab:classification-errors}
  \end{center}
\end{table}

\subsection{Penalized Logistic Regression}
As a natural next step after the logistic regression, we ran a penalized logistic regression, to see how much of the test error can be attributed to the variance in the data. The plot of logLoss is in the Fig.~\ref{fig:penLLmisses}. Other metrics evince a steady monotonical growth and have optimum at $\lambda = 0$, which tells us about high model bias and an insignificant variance in the data. Only the interesting range of $\lambda$ is shown.

\subsection{Feature transformations}
Look at the ``Data visualization and cleaning'' for the initial data preparation.

First thing we tried is {\bf removal} of every single feature, in oreder to decrease the dimensionality of the problem, to speed up future analyses. To find an irrelevant feature, we, using cross validation, sequentially fitted logistic regression on the training data set with the feature \# i removed, and tested the resulting model. Unfortunately, it turns out, that every of the 24 features is relevant, and removing any of them worsen the prediction, as you can see in the Table~\ref{tab:classification-errors}.

Having failed to reduct the feature set, we started expanding it, as our logistic regression has high bias. One of the promising methods is {\bf dummy-encoding}. There are four features, that have a finite and small discrete range. The results of expanding each of these features are shown in the Table~\ref{tab:classification-dummy}. As you can see from the table, no luck this time as well. Although, feature \#9 dummy-encoding promises a slightly (0.1\%) better prediction in terms of 0-1 loss, it is worse in other metrics, comparing to the original logistic regression.
\begin{table}[h]
  \begin{center}
    \begin{tabular}{l|ccc}
      Feature\# & RMSE & 0-1 loss & logLoss \\
      \hline
      8 & 0.3055 & 0.1233 & 0.3661 \\
      9 & 0.3051 & 0.1195 & 0.3654 \\
      10 & 0.3061 & 0.1219 & 0.3675 \\
      17 & 0.3060 & 0.1229 & 0.3659
    \end{tabular}
    \caption{The best errors for the classification methods tried.}
    \label{tab:classification-dummy}
  \end{center}
\end{table}

The last transformation tried is {\bf polynomial}. Monomial basis can theoretically approximate any function. We generate a full polynom, which means 625 features for second degree and 15625 -- for the third, which is substentially greater than the number of training data points, so due to the curse of dimensionality, the regression is unable to fit a satisfactory model. Therefore we tested only the second degree. We used cross validation with penalized logistic regression to supress the high variance of 625 features. In the Fig.~\ref{fig:polyErrors} you can see the plot of RMSE for the polynomial of the second degree.

\begin{figure}[!t]
\center
\includegraphics[width=6in]{figures/penLLmisses.pdf}
\caption{Penalized logistic regression, ran on the original set of 24 features.}
\label{fig:penLLmisses}
\end{figure}


\begin{figure}[!t]
\center
\includegraphics[width=6in]{figures/polyLogLoss.pdf}
\caption{\todo{Polygon.}}
\label{fig:polyErrors}
\end{figure}



\section{Summary}

\subsubsection*{Acknowledgments}

\subsubsection*{References}

\end{document}
