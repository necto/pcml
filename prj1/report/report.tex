\documentclass{article} % For LaTeX2e
% We will use NIPS submission format
\usepackage{nips13submit_e,times}
% for hyperlinks
\usepackage{hyperref}
\usepackage{url}
% For figures
\usepackage{graphicx} 
\usepackage{subfigure} 
% math packages
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsopn}
\usepackage{ifthen}
\usepackage{natbib}
\usepackage{color}
\usepackage{float}
\usepackage{placeins}
\usepackage{geometry}


\title{Project-I by Group Istanbul}

\author{
Johan Droz\\
EPFL \\
\texttt{johan.droz@epfl.ch} \And
Arseniy Zaostrovnykh\\
EPFL \\
\texttt{arseniy.zaostrovnykh@epfl.ch}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\nipsfinalcopy 

\newcommand{\todo}[1]{}
\renewcommand{\todo}[1]{{\color{red} TODO: {#1}}}

\begin{document}

\maketitle

\begin{abstract}
In this report we describes our result for the first project of the PCML class.
We start by doing some basic exploratory data analysis on our data, then we implement regression and classification methods to analyze and predict data.
Finally we experimented with feature engineering and processing with the goal of improving the accuracy of our models.
\end{abstract}

\section{Data Description}
\todo{probably this one shold remain present.}
\paragraph{Regression} For the regression part of this project we received a dataset containing three sets named $\boldmath{X\_train, y\_train}$ and $\boldmath{X\_test}$. The pairs $\boldmath{X\_train, y\_train}$  form our training set. 
The former contains N = 2800 sample of dimensionality D = 66 and includes 54 real, 6 binary and 6 categorical variables with a number of categories between 3 and 4.
The later contains 2800 real variables that represent the output of $\boldmath{X\_train}$ .
We also get a test set $X\_test$ of 1200 data examples for which we have to predict the output $\boldmath{y}$ as well as the expected RMSE for our best model.

\section{Data visualization and cleaning}
\todo{normalization}
\todo{\{-1, 1\} to \{0, 1\}}
\todo{this works as an addition to the previous one.}
% A figure reference example: Figure \ref{fig:boxplotX}

The first step is to perform exploratory data analysis on the regression and classification datasets.
\paragraph{Regression} 
By looking at the histograms of the input variables, we notice that there are some outliers. By removing them the rmse decrease slightly.
For example with leastSquares it decreases from 1378.8 to 1370.4.

Secondly we notice that we need to normalize the input data because their distributions are not centered.

Another interesting observation is the distribution of the output $\boldmath{y\_train}$ that is shown on figure \ref{fig:histY}.
We clearly see that the data are separated into 3 groups.

We also look into the correlation between input variables and the output. We see that out of the 66 input variables, two have a high correlation with the output.
\begin{figure}[!t]
	\centering
	\subfigure[Scatter plot of one input variable vs output]{\label{fig:scatter}\includegraphics[width=60mm]{figures/cluster.pdf}}
	\subfigure[Histogram of y\_train]{\label{fig:histY}\includegraphics[width=60mm]{figures/histY.pdf}}
	\caption{Data analysis}
\end{figure}
As we can see on figure \ref{fig:scatterIO}, the data point are grouped into 3 different clusters.


\section{Regression}
To  estimate the rmse of our model with the data in hand, we split them into training and validation sets (80\% as training data and 20\% as validation) and pretend validation set is the future data.

For all regression analyses, 7-fold cross-validation is used.

We stared by trying the three functions that were mandatory to implement and tried to improve the results using various feature transformations. The best errors for each method are shown in Table \ref{tab:regression-errors}.

The rest of this section describes the details of each of these methods.
\begin{table}[h]
	\begin{center}
		\begin{tabular}{l|c}
			Method & RMSE  \\
			\hline
			Least Squares with Gradient Descent & 1370.1   \\
			Least Squares & 1370.1  \\
			Ridge regression &  1645 \\
			1-Feature Removal &  1408.9 \\
			X-Feature Removal &  1409 \\
			Dummy-coding &  1409.3 \\
		\end{tabular}
		\caption{The best errors for the regression methods tried.}
		\label{tab:regression-errors}
	\end{center}
\end{table}


\subsection{Feature transformations}
In order to improve the rmse of our model, we tried several feature transformations.

Firstly, we try to decrease the dimensionality of the problem by sequentially removing each feature and apply cross validation to estimate the rmse with each feature removed.
Unfortunately, feature removal does not improve our model. It is actually the opposite as the rmse increase slightly.

Secondly we tried dummy coding of the categorical variables.
As explained in the "Data description" section, the Regression set's input has 6 categorical variables each with a number of categories between 3 and 4.

\begin{table}[h]
	\begin{center}
		\begin{tabular}{l|c}
			Feature\# & RMSE  \\
			\hline
			2 &  1410.2 \\
			12 & 1410.1 \\
			14 &  1409.4 \\
			29 &  1410.6 \\
			48 &  1410.2 \\
			62 &  1409.8 \\
		\end{tabular}
		\caption{RMSE for dummy coding.}
		\label{tab:regression-dummy}
	\end{center}
\end{table}

\section{Classification}

Before getting into the analysis, we first shuffle the data by pseudo-rundom permutation to get rid of inherent ordering, that might bias the model. We also average the results over a range of rundom number generator seeds. We use In order to evaluate our final model later we cut off 10\% of the training data (150 points) to simulate the unseen future input.

For all classification analyses we use 7-fold cross-validation. 

We started with the classical logistic regression \todo{Newton?}, and tried to improve the obtained result using penalized logistic regression and various feature transformations. The best errors for each method are shown in the Table~\ref{tab:classification-errors}. The rest of this section describes the details of each of them. The baseline -- trivial constant majority-based prediction is also shown for a reference.
\begin{table}[h]
  \begin{center}
    \begin{tabular}{l|ccc}
      Method & RMSE & 0-1 loss & logLoss \\
      \hline
      Majority & 0.69 & 0.47 & - \\
      Logistic Regression & 0.31 & 0.12 & 0.37 \\
      Penalized Logistic Regression & 0.34 & 0.15 & 0.36  \\
      1-Feature Removal & 0.34 & 0.17 & 0.36 \\
      Dummy-coding & 0.31 & 0.12 & 0.37 \\
      Polynomial & 0.33 & 0.17 & 0.46
    \end{tabular}
    \caption{The best errors for the classification methods tried.}
    \label{tab:classification-errors}
  \end{center}
\end{table}

\subsection{Penalized Logistic Regression}
As a natural next step after the logistic regression, we ran a penalized logistic regression, to see how much of the test error can be attributed to the variance in the data. The plot of logLoss is in the Fig.~\ref{fig:penLLmisses}. Other metrics evince a steady monotonical growth and have optimum at $\lambda = 0$, which tells us about high model bias and an insignificant variance in the data. Only the interesting range of $\lambda$ is shown.

\subsection{Feature transformations}
Look at the ``Data visualization and cleaning'' for the initial data preparation.

First thing we tried is {\bf removal} of every single feature, in oreder to decrease the dimensionality of the problem, to speed up future analyses. To find an irrelevant feature, we, using cross validation, sequentially fitted logistic regression on the training data set with the feature \# i removed, and tested the resulting model. Unfortunately, it turns out, that every of the 24 features is relevant, and removing any of them worsen the prediction, as you can see in the Table~\ref{tab:classification-errors}.

Having failed to reduct the feature set, we started expanding it, as our logistic regression has high bias. One of the promising methods is {\bf dummy-encoding}. There are four features, that have a finite and small discrete range. The results of expanding each of these features are shown in the Table~\ref{tab:classification-dummy}. As you can see from the table, no luck this time as well. Although, feature \#9 dummy-encoding promises a slightly (0.1\%) better prediction in terms of 0-1 loss, it is worse in other metrics, comparing to the original logistic regression.
\begin{table}[h]
  \begin{center}
    \begin{tabular}{l|ccc}
      Feature\# & RMSE & 0-1 loss & logLoss \\
      \hline
      8 & 0.3055 & 0.1233 & 0.3661 \\
      9 & 0.3051 & 0.1195 & 0.3654 \\
      10 & 0.3061 & 0.1219 & 0.3675 \\
      17 & 0.3060 & 0.1229 & 0.3659
    \end{tabular}
    \caption{The best errors for the classification methods tried.}
    \label{tab:classification-dummy}
  \end{center}
\end{table}

The last transformation tried is {\bf polynomial}. Monomial basis can theoretically approximate any function. We generate a full polynom, which means 625 features for second degree and 15625 -- for the third, which is substentially greater than the number of training data points, so due to the curse of dimensionality, the regression is unable to fit a satisfactory model. Therefore we tested only the second degree. We used cross validation with penalized logistic regression to supress the high variance of 625 features. In the Fig.~\ref{fig:polyErrors} you can see the plot of logLoss function for the polynomial of the second degree. The other two metrics for the given random seed again do not have a bell form.

\begin{figure}[!t]
\center
\includegraphics[width=6in]{figures/penLLmisses.pdf}
\caption{Penalized logistic regression, ran on the original set of 24 features.}
\label{fig:penLLmisses}
\end{figure}


\begin{figure}[!t]
\center
\includegraphics[width=6in]{figures/polyLogLoss.pdf}
\caption{\todo{Polygon.}}
\label{fig:polyErrors}
\end{figure}



\section{Summary}

\subsubsection*{Acknowledgments}

\subsubsection*{References}

\end{document}
